{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input/output will be stored in: s3://sagemaker-input-2020/sagemaker-input-2020\n",
      "arn:aws:iam::385500896981:role/service-role/AmazonSageMaker-ExecutionRole-20191117T164084\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import math\n",
    "import datetime\n",
    "bucket = 'sagemaker-input-2020'   # <--- specify a bucket you have access to\n",
    "prefix = 'sagemaker-input-2020'\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('Hey! You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"Hey! You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Hey! Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))\n",
    "    print(execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Application_code  Destination_code  occurrences\n",
      "0                 101               203            8\n",
      "1                 101               205            1\n",
      "2                 101               206           86\n",
      "3                 101               207            1\n",
      "4                 101               208            1\n",
      "5                 101               209           47\n",
      "6                 101               210            5\n",
      "7                 101               211           11\n",
      "8                 101               212            8\n",
      "9                 101               213            1\n",
      "10                101               215            6\n",
      "11                101               216            8\n",
      "12                101               218           50\n",
      "13                101               219            1\n",
      "14                101               220            1\n",
      "15                101               221          134\n",
      "16                102               203            1\n",
      "17                102               206            4\n",
      "18                102               209            7\n",
      "19                102               218            5\n",
      "20                102               221           13\n",
      "21                103               206            2\n",
      "22                103               209            2\n",
      "23                103               210            1\n",
      "24                103               221            3\n",
      "25                104               218            2\n",
      "26                105               201            8\n",
      "27                105               202            2\n",
      "28                105               203            8\n",
      "29                105               204            1\n",
      "..                ...               ...          ...\n",
      "216               125               202            4\n",
      "217               125               203            3\n",
      "218               125               205           11\n",
      "219               125               206           50\n",
      "220               125               209           52\n",
      "221               125               210            1\n",
      "222               125               211            2\n",
      "223               125               212            1\n",
      "224               125               215            1\n",
      "225               125               216            3\n",
      "226               125               217            1\n",
      "227               125               218           33\n",
      "228               125               221          106\n",
      "229               126               221            4\n",
      "230               127               205            1\n",
      "231               127               206            5\n",
      "232               127               209            1\n",
      "233               127               211            1\n",
      "234               127               221            7\n",
      "235               128               206            1\n",
      "236               128               221            1\n",
      "237               129               209            1\n",
      "238               129               218            2\n",
      "239               129               221            1\n",
      "240               130               203            3\n",
      "241               130               206            6\n",
      "242               130               209            3\n",
      "243               130               212            1\n",
      "244               130               218            1\n",
      "245               130               221           10\n",
      "\n",
      "[246 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "data_filename = 'updated_log_data2.csv'\n",
    "data_source = 'https://sagemaker-input-2020.s3.amazonaws.com/updated_log_data2.csv '\n",
    "\n",
    "urllib.request.urlretrieve(data_source, data_filename)\n",
    "firewall_data = pd.read_csv(data_filename, delimiter=',', usecols = ['Application_code', 'Destination_code'])\n",
    "\n",
    "column_data = pd.read_csv(data_filename, delimiter=',', dtype=str, usecols = ['Application', 'Destination Country'])\n",
    "                                                                              \n",
    "firewall_df = firewall_data[ ['Application_code', 'Destination_code'] ].groupby(['Application_code', 'Destination_code']).size().reset_index(name='occurrences')\n",
    "                                                                              \n",
    "column_df = column_data[ ['Application', 'Destination Country'] ].groupby(['Application', 'Destination Country']).size().reset_index(name='occurr')\n",
    "                                                                              \n",
    "print(firewall_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:15: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-17 05:54:27 Starting - Starting the training job...\n",
      "2020-04-17 05:54:28 Starting - Launching requested ML instances.........\n",
      "2020-04-17 05:56:00 Starting - Preparing the instances for training...\n",
      "2020-04-17 05:56:54 Downloading - Downloading input data......\n",
      "2020-04-17 05:57:55 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/scipy/_lib/_numpy_compat.py:10: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing.nosetester import import_nose\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/scipy/stats/morestats.py:12: DeprecationWarning: Importing from numpy.testing.decorators is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing.decorators import setastest\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'_ftp_port': 8999, u'num_samples_per_tree': 256, u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'_kvstore': u'dist_async', u'force_dense': u'true', u'epochs': 1, u'num_trees': 100, u'eval_metrics': [u'accuracy', u'precision_recall_fscore'], u'_num_kv_servers': u'auto', u'mini_batch_size': 1000}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'mini_batch_size': u'1000', u'feature_dim': u'1', u'num_samples_per_tree': u'30', u'num_trees': u'50'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Final configuration: {u'_ftp_port': 8999, u'num_samples_per_tree': u'30', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'_kvstore': u'dist_async', u'force_dense': u'true', u'epochs': 1, u'feature_dim': u'1', u'num_trees': u'50', u'eval_metrics': [u'accuracy', u'precision_recall_fscore'], u'_num_kv_servers': u'auto', u'mini_batch_size': u'1000'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 WARNING 140181116733248] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/879d9e5b-2074-4d90-8056-62e7e20fbeb0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'MXNET_KVSTORE_BIGARRAY_BOUND': '400000000', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'randomcutforest-2020-04-17-05-54-27-041', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-74-113.ec2.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/924ab89b-7c6f-4f26-80ac-6c16cf4c96d5', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:385500896981:training-job/randomcutforest-2020-04-17-05-54-27-041', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/879d9e5b-2074-4d90-8056-62e7e20fbeb0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'MXNET_KVSTORE_BIGARRAY_BOUND': '400000000', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.74.113', 'AWS_REGION': 'us-east-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'randomcutforest-2020-04-17-05-54-27-041', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-74-113.ec2.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/924ab89b-7c6f-4f26-80ac-6c16cf4c96d5', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:385500896981:training-job/randomcutforest-2020-04-17-05-54-27-041', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/879d9e5b-2074-4d90-8056-62e7e20fbeb0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'MXNET_KVSTORE_BIGARRAY_BOUND': '400000000', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'randomcutforest-2020-04-17-05-54-27-041', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-74-113.ec2.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/924ab89b-7c6f-4f26-80ac-6c16cf4c96d5', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:385500896981:training-job/randomcutforest-2020-04-17-05-54-27-041', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/879d9e5b-2074-4d90-8056-62e7e20fbeb0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'MXNET_KVSTORE_BIGARRAY_BOUND': '400000000', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.74.113', 'AWS_REGION': 'us-east-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'randomcutforest-2020-04-17-05-54-27-041', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-74-113.ec2.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/924ab89b-7c6f-4f26-80ac-6c16cf4c96d5', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:385500896981:training-job/randomcutforest-2020-04-17-05-54-27-041', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/879d9e5b-2074-4d90-8056-62e7e20fbeb0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '1', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'MXNET_KVSTORE_BIGARRAY_BOUND': '400000000', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.74.113', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'randomcutforest-2020-04-17-05-54-27-041', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-74-113.ec2.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/924ab89b-7c6f-4f26-80ac-6c16cf4c96d5', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:385500896981:training-job/randomcutforest-2020-04-17-05-54-27-041', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 32 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 33 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Using default worker.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Verifying hyperparamemters...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Hyperparameters are correct.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Validating that feature_dim agrees with dimensions in training data...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] feature_dim is correct.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Validating memory limits...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Available memory in bytes: 15313903616\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Estimated sample size in bytes: 12000\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Estimated memory needed to build the forest in bytes: 60000\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Memory limits validated.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Starting cluster sharing facilities...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] >>> starting FTP server on 0.0.0.0:8999, pid=1 <<<\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] >>> starting FTP server on 0.0.0.0:8999, pid=1 <<<\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] poller: <class 'pyftpdlib.ioloop.Epoll'>\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] poller: <class 'pyftpdlib.ioloop.Epoll'>\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] masquerade (NAT) address: None\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] masquerade (NAT) address: None\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] passive ports: None\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] passive ports: None\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] use sendfile(2): False\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] use sendfile(2): False\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Cluster sharing facilities started.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Verifying all workers are accessible...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] All workers accessible.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Initializing Sampler...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Sampler correctly initialized.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 669.2659854888916, \"sum\": 669.2659854888916, \"min\": 669.2659854888916}}, \"EndTime\": 1587103079.889236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.216493}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1587103079.889766, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.889506}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-04-17 05:57:59.889] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 672, \"num_examples\": 1, \"num_bytes\": 6888}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Sampling training data...\u001b[0m\n",
      "\u001b[34m[2020-04-17 05:57:59.905] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6888}\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Sampling training data completed.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"update.time\": {\"count\": 1, \"max\": 20.859956741333008, \"sum\": 20.859956741333008, \"min\": 20.859956741333008}}, \"EndTime\": 1587103079.91094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.88934}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246, \"sum\": 246.0, \"min\": 246}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 246, \"sum\": 246.0, \"min\": 246}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246, \"sum\": 246.0, \"min\": 246}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1587103079.911284, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\", \"epoch\": 0}, \"StartTime\": 1587103079.890051}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] #throughput_metric: host=algo-1, train throughput=11463.2846049 records/second\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Master node: building Random Cut Forest...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Gathering samples...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] 246 samples gathered\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Building Random Cut Forest...\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Random Cut Forest built: \n",
      "\u001b[0m\n",
      "\u001b[34mForestInfo{num_trees: 50, num_samples_in_forest: 200, num_samples_per_tree: 4, sample_dim: 1, shingle_size: 1, trees_num_nodes: [5, 7, 5, 5, 7, 5, 5, 7, 5, 5, 5, 3, 3, 5, 3, 5, 7, 7, 7, 7, 7, 5, 7, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5, 3, 7, 7, 7, 7, 7, 7, 3, 3, 5, 3, 7, 7, ], trees_depth: [3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 2, 2, 3, 2, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 4, 4, 4, 4, 3, 4, 2, 2, 3, 2, 4, 4, ], max_num_nodes: 7, min_num_nodes: 3, avg_num_nodes: 5, max_tree_depth: 4, min_tree_depth: 2, avg_tree_depth: 3, mem_size: 30192}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1.5740394592285156, \"sum\": 1.5740394592285156, \"min\": 1.5740394592285156}, \"model.bytes\": {\"count\": 1, \"max\": 30192, \"sum\": 30192.0, \"min\": 30192}, \"fit_model.time\": {\"count\": 1, \"max\": 0.5278587341308594, \"sum\": 0.5278587341308594, \"min\": 0.5278587341308594}}, \"EndTime\": 1587103079.913237, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.911021}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Master node: Serializing the RandomCutForest model\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"serialize_model.time\": {\"count\": 1, \"max\": 0.8900165557861328, \"sum\": 0.8900165557861328, \"min\": 0.8900165557861328}}, \"EndTime\": 1587103079.91426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.913327}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140181116733248] Test data is not provided.\u001b[0m\n",
      "\u001b[34m[I 20-04-17 05:57:59] >>> shutting down FTP server (0 active fds) <<<\u001b[0m\n",
      "\u001b[34m[04/17/2020 05:57:59 INFO 140179645904640] >>> shutting down FTP server (0 active fds) <<<\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 916.9318675994873, \"sum\": 916.9318675994873, \"min\": 916.9318675994873}, \"setuptime\": {\"count\": 1, \"max\": 200.2389430999756, \"sum\": 200.2389430999756, \"min\": 200.2389430999756}}, \"EndTime\": 1587103079.926166, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"RandomCutForest\"}, \"StartTime\": 1587103079.914324}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-17 05:58:08 Uploading - Uploading generated training model\n",
      "2020-04-17 05:58:08 Completed - Training job completed\n",
      "Training seconds: 74\n",
      "Billable seconds: 74\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import RandomCutForest\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                      num_samples_per_tree=30,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(firewall_df.occurrences.as_matrix().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: randomcutforest-2020-04-17-05-54-27-041\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: randomcutforest-2020-04-17-05-54-27-041\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:385500896981:endpoint/randomcutforest-2020-04-17-05-54-27-041\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a849c6b5231f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m rcf_inference = rcf.deploy(\n\u001b[1;32m      2\u001b[0m     \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m-> 2378\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m         )\n\u001b[1;32m   2380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:385500896981:endpoint/randomcutforest-2020-04-17-05-54-27-041\"."
     ]
    }
   ],
   "source": [
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(rcf_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Serialization/Deserialization\n",
    "\n",
    "We can pass data in a variety of formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted and RecordIO Protobuf. We make use of the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'application/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the training dataset, in CSV format, to the inference endpoint so we can automatically detect the anomalies we saw with our eyes in the plots, above. Note that the serializer and deserializer will automatically take care of the datatype conversion from Numpy NDArrays.\n",
    "\n",
    "For starters, let's only pass in the first six datapoints so we can see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_numpy = taxi_data.value.as_matrix().reshape(-1,1)\n",
    "print(taxi_data_numpy[:6])\n",
    "results = rcf_inference.predict(taxi_data_numpy[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Anomaly Scores\n",
    "\n",
    "Now, let's compute and plot the anomaly scores from the entire taxi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rcf_inference.predict(taxi_data_numpy)\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "taxi_data['score'] = pd.Series(scores, index=taxi_data.index)\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "#start, end = 5500, 6500\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data_subset['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(taxi_data_subset['score'], color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the anomaly score spikes where our eyeball-norm method suggests there is an anomalous data point as well as in some places where our eyeballs are not as accurate.\n",
    "\n",
    "Below we print and plot any data points with scores greater than 3 standard deviations (approx 99.9th percentile) from the mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean = taxi_data['score'].mean()\n",
    "score_std = taxi_data['score'].std()\n",
    "score_cutoff = score_mean + 3*score_std\n",
    "\n",
    "anomalies = taxi_data_subset[taxi_data_subset['score'] > score_cutoff]\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of known anomalous events which occurred in New York City within this timeframe:\n",
    "\n",
    "* `2014-11-02` - NYC Marathon\n",
    "* `2015-01-01` - New Year's Eve\n",
    "* `2015-01-27` - Snowstorm\n",
    "\n",
    "Note that our algorithm managed to capture these events along with quite a few others. Below we add these anomalies to the score plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.plot(anomalies.index, anomalies.score, 'ko')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current hyperparameter choices we see that the three-standard-deviation threshold, while able to capture the known anomalies as well as the ones apparent in the ridership plot, is rather sensitive to fine-grained peruturbations and anomalous behavior. Adding trees to the SageMaker RCF model could smooth out the results as well as using a larger data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and Delete the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To do so execute the cell below. Alternately, you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "---\n",
    "\n",
    "We used Amazon SageMaker Random Cut Forest to detect anomalous datapoints in a taxi ridership dataset. In these data the anomalies occurred when ridership was uncharacteristically high or low. However, the RCF algorithm is also capable of detecting when, for example, data breaks periodicity or uncharacteristically changes global behavior.\n",
    "\n",
    "Depending on the kind of data you have there are several ways to improve algorithm performance. One method, for example, is to use an appropriate training set. If you know that a particular set of data is characteristic of \"normal\" behavior then training on said set of data will more accurately characterize \"abnormal\" data.\n",
    "\n",
    "Another improvement is make use of a windowing technique called \"shingling\". This is especially useful when working with periodic data with known period, such as the NYC taxi dataset used above. The idea is to treat a period of $P$ datapoints as a single datapoint of feature length $P$ and then run the RCF algorithm on these feature vectors. That is, if our original data consists of points $x_1, x_2, \\ldots, x_N \\in \\mathbb{R}$ then we perform the transformation,\n",
    "\n",
    "```\n",
    "data = [[x_1],            shingled_data = [[x_1, x_2, ..., x_{P}],\n",
    "        [x_2],    --->                     [x_2, x_3, ..., x_{P+1}],\n",
    "        ...                                ...\n",
    "        [x_N]]                             [x_{N-P}, ..., x_{N}]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shingle(data, shingle_size):\n",
    "    num_data = len(data)\n",
    "    shingled_data = np.zeros((num_data-shingle_size, shingle_size))\n",
    "    \n",
    "    for n in range(num_data - shingle_size):\n",
    "        shingled_data[n] = data[n:(n+shingle_size)]\n",
    "    return shingled_data\n",
    "\n",
    "# single data with shingle size=48 (one day)\n",
    "shingle_size = 48\n",
    "prefix_shingled = 'sagemaker/randomcutforest_shingled'\n",
    "taxi_data_shingled = shingle(taxi_data.values[:,1], shingle_size)\n",
    "print(taxi_data_shingled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new training job and and inference endpoint. (Note that we cannot re-use the endpoint created above because it was trained with one-dimensional data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix_shingled),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix_shingled),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data_shingled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'appliation/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above inference endpoint we compute the anomaly scores associated with the shingled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the shingled datapoints\n",
    "results = rcf_inference.predict(taxi_data_shingled)\n",
    "scores = np.array([datum['score'] for datum in results['scores']])\n",
    "\n",
    "# compute the shingled score distribution and cutoff and determine anomalous scores\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "score_cutoff = score_mean + 3*score_std\n",
    "\n",
    "anomalies = scores[scores > score_cutoff]\n",
    "anomaly_indices = np.arange(len(scores))[scores > score_cutoff]\n",
    "\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the scores from the shingled data on top of the original dataset and mark the score lying above the anomaly score threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(scores, color='C1')\n",
    "ax2.scatter(anomaly_indices, anomalies, color='k')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with this particular shingle size, hyperparameter selection, and anomaly cutoff threshold that the shingled approach more clearly captures the major anomalous events: the spike at around t=6000 and the dips at around t=9000 and t=10000. In general, the number of trees, sample size, and anomaly score cutoff are all parameters that a data scientist may need experiment with in order to achieve desired results. The use of a labeled test dataset allows the used to obtain common accuracy metrics for anomaly detection algorithms. For more information about Amazon SageMaker Random Cut Forest see the [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
